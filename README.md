# Proyecto Transversal - ASIXcD1

**Grupo:** 2  
**Equipo:** Alex Jim√©nez, Miquel Serra, Javier Vericat, Ra√∫l Ju√°rez, Aleix Tom√°s.

## √çndice
0. [Introducci√≥n](#0-introducci√≥n)  
1. [Propuesta de CPD](#1--propuesta-de-cpd)  
2. [Implementaci√≥n de servicios de audio y v√≠deo](#2--implementaci√≥n-de-servicios-de-audio-y-v√≠deo)  
3. [Implementaci√≥n y dise√±o de una base de datos](#3--dise√±o-e-implementaci√≥n-de-una-base-de-datos)  
4. [Otros servicios](#4-otros-servicios)  
   4.1. [Servidor web (NGINX)](#41--servidor-web-nginx)  
   4.2. [Servidor DNS + autenticaci√≥n y gesti√≥n de usuarios (LDAP)](#42--servidor-dns--autenticaci√≥n-y-gesti√≥n-de-usuarios-ldap))  
   4.3. [Servicio de backups](#43--servicio-de-backups)  
6. [Sostenibilidad](#5--sostenibilidad)  
7. [Conclusi√≥n](#6-conclusi√≥n)  

## 0. Introducci√≥n
Este proyecto simula la instalaci√≥n de un peque√±o CPD, en el cual hemos desplegado servicios de gesti√≥n centralizada de usuarios (LDAP), DNS, backups y servidor web; adem√°s de los tres servicios obligatorios: v√≠deo, audio y base de datos.

Cada miembro del grupo se ha encargado de configurar una instancia de AWS en la que operan los servicios mencionados. Puedes consultar [aqu√≠](#implementaci√≥-del-cpd-al-n√∫vol-aws-amb-els-serveis-utilitzats) la distribuci√≥n de servicios asignados a cada integrante.

## 1. üóÑ Propuesta de CPD
### Ubicaci√≥n f√≠sica
- **Situaci√≥n f√≠sica de la sala del edificio.**  
  La ubicaci√≥n del CPD ser√° en una planta subterr√°nea, ya que tendr√° una temperatura constante durante todo el a√±o y permite aprovechar mejor los sistemas de refrigeraci√≥n natural.
Adem√°s, de esta manera se mejora su seguridad f√≠sica con protecci√≥n contra intrusos y se disminuyen riesgos ambientales como incendios, tormentas, etc.
Tambi√©n ser√° m√°s f√°cil minimizar el impacto ac√∫stico hacia el entorno.

- **Sistemes de climatitzaci√≥ (aire condicionat). Nivells de temperatura, humitat i neteja de l‚Äôaire.**  
  Sistema de climatizaci√≥n mixto:  
  - Sistema de free cooling indirecto (usando el aire exterior filtrado cuando las condiciones lo permitan).
  - CRAC con refrigeraci√≥n l√≠quida para mantener la temperatura entre 18-22¬∫C y la humedad entre 45%-55%.  
Temperatura operativa: 20-24‚ÄØ¬∞C.  
Sistema de filtrado HEPA para limpiar el aire de part√≠culas que puedan afectar al servidor.  

- **Mesures per dificultar la identificaci√≥ de la sala.**  
  Que no haya visi√≥n de fuera a dentro, se√±alizaci√≥n discreta para evitar la identificaci√≥n de la sala.
Acceso a la planta subterr√°nea a trav√©s de una puerta interior segura, dentro de la zona de uso t√©cnico restringido.
- **Distribuci√≥ i gesti√≥ del cablejat.**  
  Cableado estructurado en canales, organizado por categor√≠as (cat 6, cat 7, corriente etc‚Ä¶)
- **Terra t√®cnic i sostre t√®cnic.**  
Los cables y tubos ir√°n por debajo de un suelo t√©cnico con una elevaci√≥n de 30 cm para facilitar el paso de cables y aire. Tambi√©n se har√° un techo t√©cnico con el mismo fin.
- **Planells, dibuixos, diagrames dels elements anteriorment citats.**  
![plano2d](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/plano2d.png)
![plano3d](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/plano3d.png)  
- **Estructuraci√≥ dels racks (m√≠nim 2 racks).**

  **Rack #1**
  
  Funci√≥n: Alberga los servidores f√≠sicos y servicios virtualizados (web, v√≠deo, audio, BBDD, DNS, DHCP, backups).

  U1 ‚Äì U2: Sistema de alimentaci√≥n  
U3 ‚Äì U4: SAI  
U5 ‚Äì U7: Servidor f√≠sico 1 (virtualitzaci√≥ principal)  
U8 ‚Äì U10: Servidor f√≠sico 2 (base de datos y backups)  
U11 ‚Äì U13: Paneles ciegos (espacio reservado a posibles ampliaciones)  
U14: Switch  
U16: Patch panel  
U17 ‚Äì U24: Espacio tapado con paneles ciegos para mejorar circulaci√≥n de aire  

  **Rack #2**
  
  Funci√≥n: Comunicaci√≥n y acceso a nivel de red.
  
  U1 ‚Äì U2: Sistema de alimentaci√≥n  
U3 ‚Äì U4: SAI  
U5 ‚Äì U6: Enrutador  
U7 ‚Äì U8: Switch  
U9: Consola KVM (para poder acceder localmente a los servidores en caso de fallo de red)  
U10 ‚Äì U11: Patch panel  
U12 ‚Äì U24: Espacio tapado con paneles ciegos para mejorar circulaci√≥n de aire y reservado para posibles ampliaciones.  


### Infraestructura IT:
- **Servidors: N√∫mero i tipus de model**  
Cantidad: 2 servidores f√≠sicos  
Modelo: Dell PowerEdge R750 (2U)
- **Patch panels.**  
Cantidad: 2 patch panels (1 por rack)  
Modelo: Aiten Data Patch Panel 24 Puertos FTP Cat 6 (1U)  
- **Switches.**  
Cantidad: 2 switches  
Modelo: Cisco Business CBS350-24T-4G (1U)  
- **Enrutador.**  
Cantidad: 1 router  
Modelo: Cisco C1131-8PWE (1U)  
- **Planells i diagrames de com estan distribu√Øts els racks amb els servidors, patch panels i switches.**

![racks](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/racks.png)

### Infraestructura el√®ctrica: 
- **Sistemes d‚Äôalimentaci√≥ redundant.**  
La instalaci√≥n el√©ctrica se har√≠a con una configuraci√≥n N+1 para garantizar que siempre hay suministro el√©ctrico.  
Esta configuraci√≥n consiste en que toda fuente de alimentaci√≥n, transformadores etc tengan otra fuente de alimentaci√≥n de respaldo para reducir dr√°sticamente las posibilidades de quedarse sin corriente.  
- **SAIS.**    
Nuestro CPD tendr√° que tener SAIS lo suficientemente grandes como para aguantar 1 hora, que se estima que es un tiempo razonable como para transicionar a los generadores de emergencia.  
Tendr√° dos SAI (uno por cada rack).  
Adem√°s los SAIS tienen la funci√≥n de regular la tensi√≥n de la red, para no estropear el CPD con sobretensi√≥n.
Falta saber cu√°nto consumen los CPDS para calcular la capacidad del SAIS

| Dispositivo                | Modelo                      | Consumo estimado (W)      |
|---------------------------|-----------------------------|---------------------------|
| Servidor Dell PowerEdge R750 | 2 unidades                  | 700 W c/u (1400 W)         |
| Switch Cisco CBS350-24T-4G   | 2 unidades                  | 30 W c/u (60 W)            |
| Patch Panel               | 2 unidades                  | 0 W (pasivo)              |
| Consola KVM               | 1 unidad                    | 40 W                      |
| Enrutador                 | 1 unidad                    | 50 W                      |
| Otros (ventiladores, etc.) | Estimaci√≥n por rack         | 100 W por rack (200 W)    |
|                           |                             |                           |
| **Total estimado**        |                             | **1750 W**                |

Para 1 hora de funcionamiento sin corriente:  
Energ√≠a necesaria = Potencia (W) √ó Tiempo (h)  
Energ√≠a = 1750 W √ó 1 h = 1750 Wh = 1.75 kWh  

### Seguretat f√≠sica i l√≤gica:  
- **F√≠sica:**  
  - **Elements de control d‚Äôacc√©s a incorporar en el CPD.**  
En la planta subterr√°nea pondremos acceso por:  
    - Huella dactilar  
    - Tarjeta de proximidad  
Para tener as√≠ una doble seguridad en casa de atracos.  
  - **Videovigil√†ncia.**  
C√°maras de videovigilancia ip 24/7 y acceso remoto a las c√°maras. Lo pondremos en la entrada y por el interior del CPD.  
  - **Sistemes prevenci√≥, detecci√≥ i d‚Äôextinci√≥ d‚Äôincendis.**  
    - Sistemas de humo con sensores t√©rmicos y √≥pticos.  
    - Sistema d‚Äôextinci√≥ autom√†tica amb gas inert (FM-200), ideal para la protecci√≥n para equipos electricos y electronicos, ademas nada de polvo, aceites ni humedad ni da√±ino para las personas y equipos.  
    - Alarma sonora y visual.  
  - **Vies d‚Äôevacuaci√≥.**  
    - Salida de emergencia se√±alizada con iluminaci√≥n aut√≥noma.  
    - Mapa de fuga visible y formaci√≥n al personal con simulacros peri√≥dicos.  

  - **Diagrames, planells i fotografies de tota la seguretat f√≠sica 
incorporada.**  
En el plano de la planta subterranea se mostrar√° lo siguiente:  
    - C√°maras de videovigilancia.  
    - Acceso mediante huella dactilar.  
    - Salida de emergencia.  
    - Posici√≥n de sensores y extintores.  

![seguridad](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/plano-seguridad.png)  

- **L√≤gica:**  
  - **Restricci√≥ d‚Äôacc√©s per autoritzaci√≥.**  
Cada sistema tindr√† acc√©s per rols, amb autenticaci√≥ multifactor (MFA). Nom√©s el personal t√®cnic autoritzat podr√† accedir a determinades m√†quines o configuracions cr√≠tiques.  
  - **Firewalls.**
Firewall de nueva generaci√≥n (NGFW), como Fortinet, con inspecci√≥n profunda de paquetes.
  - **Monitorizaci√≥.**
Sistemas como Zabbix para controlar servidores, tr√°fico y estado de servicios.
Alertas autom√°ticas por correo o mensajer√≠a ante anomal√≠as.
    - **C√≤pies de seguretat/Backups.** 
Backups diarios en discos locales redundantes y en Amazon S3 (CPD en la nube).
Estrategia 3-2-1: 3 copias, 2 formatos distintos, 1 fuera del edificio.
  - **RAIDs.**  
Sistemas configurados con RAID 10, ofreciendo redundancia y altas prestaciones de escritura y lectura.  

- **Prevenci√≥ de riscos laborals:**  
  - **Mesures aplicades en mat√®ria de prevenci√≥ de RRLL en el CPD**  
    - Se√±alizaci√≥n adecuada de salidas, materiales peligrosos y normas de emergencia.  
    - Extintores manuales y alfombras antielectrost√°ticas.  
    - Iluminaci√≥n de emergencia y sistema de alarma.  
    - EPI (Equipos de Protecci√≥n Individual): guantes, zapatos antideslizantes, chaleco.  
    - Formaci√≥n obligatoria para el personal t√©cnico en riesgos el√©ctricos y protocolos de evacuaci√≥n.  

### Sostenibilitat:
- **Com optimitzar el consum d‚Äôenergia.**  
  - Virtualizar servidores para reducir el n√∫mero de m√°quinas f√≠sicas  
  - Sistemas de monitorizaci√≥n como el Zabbix  
  - Consolidaci√≥n de cargas para maximizar el uso de los recursos  
- **√ös d‚Äôenergia verda pel CPD.**  
  El CPD se conectar√° preferentemente a fuentes de energ√≠a renovable, como la fotovoltaica, a trav√©s de proveedores verdes o con una instalaci√≥n propia de placas solares. Esto permitir√° alimentar a los sistemas con energ√≠a limpia y reducir dr√°sticamente la huella de carbono. 
- **Estalvi en longitud de cablejat.**  
  - Reducir el material necesario.  
  - Minimizar p√©rdidas energ√©ticas por distancia.  
  - Mejorar la refrigeraci√≥n pasiva en el espacio t√©cnico.  

- **Sistemes de circulaci√≥ d‚Äôaire que aprofitin condicions naturals.**  
  - Sistemas de free cooling, que utilizan aire exterior cuando es m√°s fr√≠o que el interior.  
  - Circulaci√≥n de aire canalizada y pasiva con sistemas de sobrepresi√≥n.  
  - Climatizaci√≥n con sistemas HVAC de alta eficiencia y sensores de temperatura y humedad inteligentes.  
- **Parada d‚Äôequips de comunicacions quan no hi ha c√†rrega.**  
  - Switches, equipos de r	ed y servidores secundarios cuando no haya carga o fuera de horas punta.  
  - Sistemas de comunicaci√≥n redundantes no esenciales en horas de bajo uso.  
- **Equips de baix consum energ√®tic.**  
  - Servidores con fuentes de alimentaci√≥n de alto rendimiento. 
  - Discos SSD (m√°s eficientes que HDD).  
  - Racks y componentes con mejor ventilaci√≥n y menor necesidad de refrigeraci√≥n activa.  

### Implementaci√≥ del CPD al n√∫vol AWS amb els serveis utilitzats.

Montaremos estos servicios por cada MV:  
- MV1 ‚Üí Servidor de v√≠deo + audio (Miquel)
- MV2 ‚Üí Servidor web (Javier)
- MV3 ‚Üí Base de datos (Ra√∫l)
- MV4 ‚Üí DNS + autenticaci√≥n y gesti√≥n de usuarios ‚ÄúLDAP‚Äù (Alex)
- MV5 ‚Üí Backups (Aleix)  


### Investigar i comparar efici√®ncia energ√®tica amb altres prove√Ødors del n√∫vol.  

**Amazon Web Services (AWS)**  
AWS ha anunciado su compromiso de utilizar energ√≠a 100% renovable antes de 2025. Sus centros de datos utilizan tecnolog√≠as como los procesadores Graviton, que ofrecen una mejor eficiencia energ√©tica, y sistemas de refrigeraci√≥n pasiva como el free cooling. Adem√°s, permiten gestionar la carga de manera autom√°tica con servicios como CloudWatch y Lambda, lo que facilita detener instancias sin uso y optimizar el consumo. Tambi√©n ofrece servicios de almacenamiento como EBS y S3, que ayudan a reducir la dependencia de infraestructura f√≠sica.  

**Google Cloud Platform (GCP)**  
Google Cloud ya funciona con energ√≠a 100% renovable desde 2017 y ha sido neutro en emisiones desde 2007. Utiliza inteligencia artificial para optimizar la climatizaci√≥n de sus centros de datos, reduciendo el consumo energ√©tico hasta un 30%. Con un PUE muy bajo (entre 1.1 y 1.2), demuestra una gran eficiencia energ√©tica. A nivel funcional, ofrece servicios similares a AWS para la virtualizaci√≥n, monitorizaci√≥n y automatizaci√≥n de recursos, destacando Stackdriver para el control en tiempo real.  

**Microsoft Azure**  
Microsoft Azure tambi√©n ha marcado objetivos muy ambiciosos en materia de sostenibilidad: quiere ser carbon negative en 2030 y eliminar todas sus emisiones hist√≥ricas para 2050. Sus centros de datos utilizan refrigeraci√≥n l√≠quida y energ√≠a renovable, con una gran eficiencia en el dise√±o t√©rmico. Ofrece servicios de apagado autom√°tico de instancias mediante Azure Monitor y herramientas como Power Automate para gestionar la carga de manera inteligente.  

| Proveedor        | Energ√≠a renovable  | Eficiencia energ√©tica        | Refrigeraci√≥n            | Automatizaci√≥n      | Compromiso ambiental     |
| ---------------- | ------------------ | ---------------------------- | ------------------------ | ------------------- | ------------------------ |
| **AWS**          | 100% (2025)        | Alta (Graviton, PUE <1.2)    | *Free cooling*           | CloudWatch + Lambda | Compensaci√≥n de CO‚ÇÇ      |
| **Google Cloud** | 100% (actualmente) | Muy alta (IA, PUE <1.1)      | Optimizada por IA        | Stackdriver         | Neutro desde 2007        |
| **Azure**        | 100% (2025)        | Alta (refrigeraci√≥n l√≠quida) | *Free cooling* + l√≠quida | Azure Monitor       | *Carbon negative* (2030) |


## 2. üéõ Implementaci√≥n de servicios de audio y v√≠deo  

### 2.1. üéß Instalaci√≥n de servidor de audio

*sudo apt update  
sudo apt install -y icecast2 darkice vnstat iftop iperf3 ufw* 

No cambio nada de la config por defecto de icecast 
![str1](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/str01.png)  

Cambio  el kernel  
![kernel1](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/kernel1.png)  
![kernel2](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/kernel2.png)  

#### Activar Icecast2
*sudo sed -i 's/ENABLE=false/ENABLE=true/' /etc/default/icecast2*

#### Configuraci√≥ DarkIce
*[general]  
duration        = 0  
bufferSecs      = 5  
reconnect       = yes*  

*[input]  
device          = default  
sampleRate      = 44100  
bitsPerSample   = 16  
channel         = 2*  

*[icecast2-0]  
bitrateMode     = vbr  
bitrate         = 128  
server          = localhost  
port            = 8000  
password        = sourcepass  
mountPoint      = live.mp3  
name            = Stream d'√†udio*  

A√±ado quality y format  
![str3](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/str03.png)  

#### Obrim els ports del firewall  
*sudo ufw allow 8000/tcp  
sudo ufw allow 22/tcp*

#### Iniciem els serveis  
*sudo systemctl restart icecast2  
sudo systemctl enable icecast2*


Instalar ffmpeg  
creo un script para que el mp3 funcione en bucle
![str4](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/str04.png)  
![str5](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/str05.png)  

SERVIDOR:
*sudo darkice -c /etc/darkice.cfg*  
uno de los dos: 
- *ffmpeg -re -i tralalerito.mp3 -vn -c:a libmp3lame -b:a 128k -content_type audio/mpeg   -f mp3 icecast://source:hackme@54.236.209.174:8000/live.mp3*  

- ./audio.sh tralalerito.mp3 

CLIENTE:
http://54.236.209.174:8000/live.mp3

### 2.2. üìπ Instalaci√≥n de servidor de v√≠deo  

#### Instal¬∑laci√≥ GStreamer i eines de v√≠deo  
*sudo apt update  
sudo apt install -y gstreamer1.0-tools gstreamer1.0-plugins-base \  
  gstreamer1.0-plugins-good gstreamer1.0-plugins-bad \  
  gstreamer1.0-plugins-ugly gstreamer1.0-libav \  
  gstreamer1.0-alsa gstreamer1.0-pulseaudio \  
  vnstat iftop iperf3 v4l-utils*  

#### Instal¬∑laci√≥ GStreamer finalitzada  
**Obrint port per RTP**  
*sudo ufw allow 5000/udp  
sudo ufw allow 5004/udp  
sudo ufw allow 22/tcp*  

Cambio tambi√©n los security groups en aws para abrir estos puertos  
![str6](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/str06.png)  

Cambio config de icecast para a√±adir un mount point  
![str7](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/str07.png)  

reinicio icecast  
ejecuto:  
*ffmpeg -re -i popeye.mp4   -c:v libvpx -b:v 800k   -c:a libvorbis -b:a 128k   -f webm   -content_type video/webm   icecast://source:hackme@54.236.209.174:8000/stream.webm*  
accedo a la web ‚Äúhttp://54.236.209.174:8000/stream.webm‚Äù  
![str8](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/str08.png)  

script para v√≠deo  
![str9](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/str09.png)  

ejecutar *./video.sh popeye.mp4*  

#### 2.3. Comprobaci√≥n del ancho de banda con iperf3  
1. Instal¬∑laci√≥:  
*sudo apt update  
sudo apt install -y iperf3*  

2. Per fer la prova al servidor (receptor): *iperf3 -s*  
3. Sortida esperada: *Server listening on 5201*  
![str10](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/str10.png)  

4. Al client (emissor): Substitueix IP_SERVIDOR per l‚Äôadre√ßa IP del servidor:  
*iperf3 -c IP_SERVIDOR*  
5. Sortida esperada:  
Connecting to host IP_SERVIDOR, port 5201  
*[ ID] Interval           Transfer     Bandwidth  
[  5]   0.00-1.00   sec   112 MBytes   938 Mbits/sec*  
![str11](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/str11.png)  


## 3. üíΩ Dise√±o e implementaci√≥n de una base de datos

### 3.1. Dise√±o entidad-relaci√≥n:  

![er](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/er.jpg)  

---

### 3.2. Modelo relacional:  

DEPARTAMENTO (id, nombre, tel√©fono)  

EMPLEADO (DNI, nombre, ap1, ap2, direcci√≥n, tel√©fono, idDep, idGrupo)  
*idDep referencia a DEPARTAMENTO;*  
*idGrupo referencia a GRUPO-NIVEL;*  

GRUPO-NIVEL (id, salario, periodoPrueba, diasVacaciones)  

---

### 3.3. Implementaci√≥n en mysql:  

*(para acceder a la db como root entramos con ‚Äúmysql -u root -p‚Äù y la contrase√±a es ‚Äúitb‚Äù).*  
![image3](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/image3.png)  

*Los datos de salarios se han introducido en base a las tablas salariales de 2024. Los d√≠as de vacaciones y el periodo de prueba en base al ‚Äúconvenio colectivo estatal de empresas de consultor√≠a, tecnolog√≠as de la informaci√≥n y estudios de mercado y de la opini√≥n p√∫blica‚Äù publicado el 16/04/2025.*  

Tablas salariales 2024:  
![tablasalarial](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/tablasalarial.png) 

Periodo de prueba:  
![periodoprueba](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/periodoprueba.png)  

D√≠as vacaciones:  
![vacaciones](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/vacaciones.png)  

Insertamos datos para los departamentos y las categor√≠as profesionales:  
![image9](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/image9.png)  

Insertamos datos para cada empleado:  
![image11](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/image11.png)  

Comprobamos que se han insertado correctamente los datos:  
![image1](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/image1.png)  
![image13](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/image13.png)  

Creamos un usuario para que los clientes puedan acceder a la base de datos con √©l y administrarla:  
![image17](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/image17.png)  

Habilitamos ‚Äúbind-address‚Äù para que cualquier IP pueda conectarse a la database:  
![image8](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/image8.png)  

Y con esto cualquier cliente dentro del rango de IPs que permita el security group ya podr√° conectarse a la base de datos.  
![secgroupDB](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/secgroupDB.png)  

--

Creamos distintos usuarios con sus roles y permisos:
![db21](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/db21.png)  

Hacemos la prueba con el usuario ‚Äòifarre‚Äô para comprobar que solo puede hacer ‚Äòselect‚Äô en la base de datos y no tiene permiso para realizar un ‚Äòinsert‚Äô, por ejemplo:
![db22](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/db22.png)  


## 4. Otros servicios  

### 4.1. üåê Servidor web (NGINX)  

Instalamos Nginx:  
![web1](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/web1.png)  

El objetivo inicial era desplegar un servidor web accesible por HTTPS en una instancia EC2 de AWS, utilizando Nginx como servidor web para alojar una web HTML que inclu√≠a audio y v√≠deo en streaming.  

Durante el desarrollo, detectamos que el servidor de streaming (que emit√≠a audio y v√≠deo en http://54.236.209.174:8000/) no soportaba HTTPS, lo que generaba bloqueos por contenido mixto en los navegadores modernos al intentar integrarlo en una p√°gina servida por HTTPS.  

Para mantener la compatibilidad con los streams y evitar bloqueos, decidimos servir toda la web en HTTP, desactivando el redireccionamiento a HTTPS. De esta forma, tanto el contenido web como el streaming funcionan correctamente. Finalmente, qued√≥ de esta manera la configuraci√≥n del Nginx:  
![web2](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/web2.png)  

Y este es el resultado de la web despu√©s de aplicar el html y el css:  
![web3](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/web3.png)  
![web4](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/web4.png)  
![web5](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/web5.png)  
![web6](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/web6.png)  
![web7](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/web7.png)  
![web8](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/web8.png)  
![web9](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/web9.png)  

### 4.2. üë• Servidor DNS + autenticaci√≥n y gesti√≥n de usuarios (LDAP)  

#### üîπ Configuraci√≥n del Servidor DNS (BIND9)

**Instalaci√≥n de BIND9**

```bash
sudo apt-get update
sudo apt-get install bind9
```
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns01.png)  

**Archivos de configuraci√≥n clave**

- `/etc/bind/named.conf`: Archivo principal que incluye otros archivos de configuraci√≥n.
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns02.png)  
- `/etc/bind/named.conf.options`: Define opciones globales como los forwarders (reenviadores).
- `/etc/bind/named.conf.local`: Define las zonas DNS locales.
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns03.png)  
- Archivos de zona: `db.asixcd-g2` y `db.172.16.3`, que contienen registros directos e inversos.
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns04.png)  
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns05.png)  
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns06.png)  
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns07.png)  
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns08.png)  
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns09.png)  


**Ejemplo de `named.conf.options`**

```
options {
    directory "/var/cache/bind";

    forwarders {
        8.8.8.8;
        1.1.1.1;
    };

    dnssec-validation auto;
    auth-nxdomain no;
    listen-on-v6 { any; };
};
```


**Ejemplo de `named.conf.local`**

```
zone "asixcd-g2.local" {
    type master;
    file "/etc/bind/db.asixcd-g2";
};

zone "3.16.172.in-addr.arpa" {
    type master;
    file "/etc/bind/db.172.16.3";
};
```


**Ejemplo de archivo `db.asixcd-g2`**

```
$TTL    604800
@       IN      SOA     dns.asixcd-g2.local. root.asixcd-g2.local. (
                          2         ; Serial
                     604800         ; Refresh
                      86400         ; Retry
                    2419200         ; Expire
                     604800 )       ; Negative Cache TTL

@       IN      NS      dns.asixcd-g2.local.
dns     IN      A       172.16.3.254
web     IN      A       172.16.4.74
vidaud  IN      A       172.16.1.112
db      IN      A       172.16.2.222
backup  IN      A       172.16.5.77
```


**Ejemplo de archivo `db.172.16.3`**

```
$TTL    604800
@       IN      SOA     dns.asixcd-g2.local. root.asixcd-g2.local. (
                          2         ; Serial
                     604800         ; Refresh
                      86400         ; Retry
                    2419200         ; Expire
                     604800 )       ; Negative Cache TTL

@       IN      NS      dns.asixcd-g2.local.
254     IN      PTR     dns.asixcd-g2.local.
```


**Reiniciar el servicio DNS**

```bash
sudo systemctl restart bind9
```

---

### Configuraci√≥n de Autenticaci√≥n Centralizada con LDAP

**Instalaci√≥n del cliente LDAP**

```bash
sudo apt-get update
sudo apt-get install nslcd libnss-ldap libpam-ldap ldap-utils
```
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns10.png)  


**Archivo `/etc/nslcd.conf`**

```
uri ldap://172.16.3.254
base dc=asixcd-g2,dc=local
binddn cn=admin,dc=asixcd-g2,dc=local
bindpw itbitb
```
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns11.png)  
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns12.png)  


**Reiniciar el servicio `nslcd`**

```bash
sudo systemctl restart nslcd
```
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns13.png)  


**Ejemplo de archivo `usuario.ldif`**

```
dn: uid=josep,ou=People,dc=asixcd-g2,dc=local
objectClass: inetOrgPerson
objectClass: posixAccount
objectClass: shadowAccount
uid: josep
sn: Josep
cn: Josep Vidaud
userPassword: {CRYPT}itbitb
uidNumber: 10001
gidNumber: 10000
homeDirectory: /home/josep
loginShell: /bin/bash
```


**Importar el usuario al servidor LDAP**

```bash
ldapadd -x -D "cn=admin,dc=asixcd-g2,dc=local" -W -f usuario.ldif
```
![dns-image](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/dns14.png)  

### 4.3. üîê Servicio de backups  

#### Introducci√≥n

En este caso nos basamos en hacer copias de seguridad, con lo cual haremos el backup de nuestra base de datos y tambi√©n alg√∫n a√±adido extra.

#### Instalaci√≥n del cliente MySQL

Vamos a instalar el cliente de **MySQL**, que nos sirve tambi√©n para hacer backups, ya que tambi√©n es m√°s accesible y podremos acceder remotamente a la base de datos.

![backup](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/backup1.png)
```bash
sudo apt install mysql-client
```

Si nos conectamos con el **host**, **usuario** y **password** correspondiente, de nuestra base de datos accederemos de manera exitosa.

![backup](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/backup2.png)
```bash
mysql -h 172.16.X.X -u usuario -p
```
---

#### Crear una copia de seguridad

Ahora vamos a hacer una copia de seguridad de la base de datos `cpd_dg2`.

![backup](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/backup3.png)
```bash
SHOW DATABASES;
```

Primero de todo, tenemos que tener **todos los permisos correspondientes**, porque a la hora de hacer la copia de seguridad nos saldr√°n errores una tras otra.

![backup](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/backup4.png)
```bash
SHOW GRANTS for usuario;
```

Este es el comando para hacer una copia de seguridad. En resumen quiere decir:

- hacer copia de seguridad
- con el usuario
- con password (que nos la pedir√° luego)
- el host o IP
- seleccionar todas las bases de datos o una en concreto
- definir la ruta de guardado
- a√±adir la fecha en que se hizo la copia

![backup](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/backup5.png)
**Ejemplo de comando:**  

```bash
mysqldump -u usuario -p -h host --basedatos nombre_basedatos > /ruta/backup_$(date +%Y-%m-%d).sql
```

#### Comprobacion y a√±adidos

Para comprobar que se hizo correctamente, buscamos la ubicaci√≥n del backup, vemos que esta ahi.
![backup](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/backup6.png)
```bash
ls -la /ruta/
```

Vemos que si hacemos si hacemos un nano o un cat del sql, vemos que se hizo correctamente.
![backup](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/backup7.png)
```bash
nano /ruta/backup_$(date +%Y-%m-%d).sql
```

Ademas hemos automatizado la copia de seguriddad con la comanda crontab -e, que automaticamente podremos hacer automatizaciones de todo tipo. En nuestro caso hicimos de esa manera

- hacer copia di√°rias a kas 23:59h
- accion correspondiente (mysqldump)
- seleccionar la base de datos
- definir la ruta de guardado
- a√±adir la fecha en que se hizo la copia

![backup](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/backup8.png)

**Ejemplo de comando:**  

```bash
59 23 * * * mysqldump --databases nommbre_basedatos > /ruta/backup_$(date +\%F_\%H-\%M-\%S).sql
```

Esto es un recurso autom√°tico que utiliza el usuario, password y host para poder conectar autom√°ticamente cuando hacemos el comando mysql para conectar nuestra base de datos, esto enlaza con √©l
![backup](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/backup9.png)
```bash
[client]
user=usuario
password=tucontrase√±asegura
host=172.16.X.X
```



## 5. üå≥ Sostenibilidad  

### Identificaci√≥n de recursos empleados.

- **1. Ubicaci√≥n sugerida del centro de operaciones**  
  - Lugar: Parque Tecnol√≥gico del Vall√®s (Cerdanyola del Vall√®s, Barcelona)  
  - Justificaci√≥n:  
    - Proximidad a Barcelona, con buena conexi√≥n de transporte p√∫blico.  
    - Infraestructuras ya preparadas para empresas tecnol√≥gicas.  
    - Acceso a servicios energ√©ticos sostenibles y fibra √≥ptica de alta capacidad.  
    - Presencia de otras empresas tecnol√≥gicas y centros de I+D.  

  - Costes aproximados:  
    - Alquiler de oficinas para 100 personas (~800 m¬≤): 9.600 ‚Ç¨/mes (12 ‚Ç¨/m¬≤)  
    - Costes de electricidad (~0,18 ‚Ç¨/kWh √ó 212 kWh √ó 30 d√≠as): ~1.145 ‚Ç¨/mes  
    - Costes operativos varios (internet, mantenimiento, climatizaci√≥n): ~2.000 ‚Ç¨/mes  
    - **Total mensual estimado:** 12.745 ‚Ç¨  
    - **Total anual estimado:** ~152.940 ‚Ç¨  
    
- **Infraestructura IT desplegada**  

| Tipo de servidor       | Funci√≥n                  | Recursos asignados              |
|------------------------|--------------------------|----------------------------------|
| Servidor de Backups    | Almacenamiento y recuperaci√≥n | 2 vCPU, 4 GB RAM, 30 GB (m√°x en cuenta estudiante AWS)   |
| Servidor V√≠deo+Audio   | Streaming multimedia      | 4 vCPU, 8 GB RAM, 30 GB      |
| Servidor Base de Datos | MySQL                | 2 vCPU, 4 GB RAM, 30 GB    |
| Servidor Web           | Hosting web y APIs        | 2 vCPU, 4 GB RAM, 30 GB    |
| Servidor DNS y LDAP  | DNS y Autenticaci√≥n y gesti√≥n de usuarios ‚ÄúLDAP‚Äù  | 2 vCPU, 4 GB RAM, 30 GB  |

- **Infraestructura f√≠sica**  

| Dispositivo     | Cantidad | Modelo                             | Rack              |
|------------------|----------|------------------------------------|--------------------|
| Servidor f√≠sico  | 2        | Dell PowerEdge R750 (2U)           | Rack #1 (U5‚ÄìU10)   |
| Switches         | 2        | Cisco CBS350-24T-4G (1U)           | Rack #1 y #2       |
| Patch panels     | 2        | Aiten Cat 6 FTP 24 puertos (1U)    | Rack #1 y #2       |
| Enrutador        | 1        | Cisco C1131-8PWE                   | Rack #2            |
| Consola KVM      | 1        | Gen√©rica                           | Rack #2            |
| SAI (UPS)        | 2        | APC Smart-UPS (~1,2 kWh)           | Ambos racks        |


- **2. Estimar el consumo energ√©tico y la huella de carbono**
  - Previsi√≥n de uso y tr√°fico mensual:  
Funcionamiento estimado: 240 horas/mes (12 h/d√≠a, 5 d√≠as/semana)  
Usuarios previstos: ~100  
Tr√°fico estimado:  
    - Web y BBDD: 1 TB  
    - V√≠deo y Audio: 3 TB  
    - Total tr√°fico mensual: 4 TB  

  - C√°lculo del consumo energ√©tico y huella de carbono

    | Componente              | Consumo mensual estimado (kWh) |
    |-------------------------|-------------------------------:|
    | Servidor f√≠sico 1       | 90                            |
    | Servidor f√≠sico 2       | 75                            |
    | Switches (2x)           | 10                            |
    | Router                  | 5                             |
    | Patch panels + KVM      | 2                             |
    | SAIs (UPS)              | 10                            |
    | Tr√°fico red (4 TB @5 W/GB) | 20                         |
    | **Total**               | **212 kWh**                   |

  - Emisiones estimadas (Espa√±a, 2024):
    - Factor conversi√≥n: 0,23 kg CO‚ÇÇ / kWh
    - Emisiones mensuales: 212 √ó 0,23 = 48,76 kg CO‚ÇÇ
    - Emisiones anuales: 48,76 √ó 12 = 585,12 kg CO‚ÇÇ/a√±o
   
### Propuesta de medidas de reducci√≥n u optimizaci√≥n:

1. **Instalaci√≥n de placas solares fotovoltaicas**
Instalaci√≥n recomendada: 12 kWp  
Producci√≥n estimada en Barcelona: ~1.700 kWh/kWp/a√±o ‚Üí 20.400 kWh/a√±o  
Cobertura del consumo estimado (2.544 kWh/a√±o): ~100%  
Emisiones evitadas: 2.544 √ó 0,23 = 585,12 kg CO‚ÇÇ/a√±o  
Ahorro el√©ctrico anual: 2.544 √ó 0,18 ‚Ç¨/kWh = 457,92 ‚Ç¨

2. **Gesti√≥n eficiente del agua**
Grifos con sensores de infrarrojos  
Sistemas de doble descarga en sanitarios  
Reutilizaci√≥n de aguas grises en limpieza y riego

3. **Optimizaci√≥n t√©rmica y energ√©tica**
Paneles ciegos en racks  
Flujo de aire optimizado (frontal-trasero)  
Aislamiento t√©rmico en salas t√©cnicas  
Termostatos inteligentes y sensores de CO‚ÇÇ

4. **Buenas pr√°cticas IT**  
Consolidaci√≥n de m√°quinas virtuales  
Autoscaling de recursos cloud  
Apagado autom√°tico fuera del horario laboral  
Migraci√≥n de servicios a AWS Irlanda (>90% renovable)  

5. **Sensibilizaci√≥n y formaci√≥n**  
Talleres y formaciones sobre eficiencia energ√©tica y sostenibilidad  
Paneles informativos con consumo energ√©tico en tiempo real

**Reparto del consumo energ√©tico mensual (kWh)**  

![grafico1](https://github.com/MiquelSerra-ITB2425/pro-ASIXcD1-g2/blob/main/images/grafico1.png)  


## 6. Conclusi√≥n  

En este proyecto hemos desplegado un CPD virtual distribuido sobre AWS, en el que hemos implementado servicios de gesti√≥n centralizada de usuarios (LDAP), resoluci√≥n de nombres (DNS), base de datos MySQL con copias de seguridad automatizadas mediante crontab y streaming de v√≠deo y audio reproducidos en una web configurada con NGINX.  

Inicialmente, para conectar las m√°quinas virtuales habr√≠amos preferido una topolog√≠a en estrella, donde una de las instancias actuara como router central. Sin embargo, debido al funcionamiento de las VPC de AWS y las restricciones de sus cuentas educativas eso no ha sido posible y hemos optado por una topolog√≠a en malla, en la que cada MV est√° conectada directamente con las dem√°s.  

En definitiva, el proyecto nos ha ayudado a comprender qu√© elementos son necesarios para que un CPD funcione correctamente, desde la infraestructura f√≠sica y l√≥gica hasta la gesti√≥n de servicios en red. Adem√°s, nos ha ayudado a mejorar nuestra capacidad de trabajo en equipo y de toma de decisiones conjunta.
